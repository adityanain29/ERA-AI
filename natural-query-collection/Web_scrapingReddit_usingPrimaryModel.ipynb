{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae65131-1372-4d8d-b57a-5a8e43049d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e51a03-f528-4c3d-a759-c8adb06bd612",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level = logging.INFO, format = '%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c93f4d0f-7a92-4b26-9783-3e04a5a5a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nbformat\n",
    "# from nbconvert import PythonExporter\n",
    "\n",
    "# with open('model_training.ipynb', 'r', encoding='utf-8') as f:\n",
    "#     nb_content = nbformat.read(f, as_version=4)\n",
    "\n",
    "# exporter = PythonExporter()\n",
    "\n",
    "# script, _ = exporter.from_notebook_node(nb_content)\n",
    "\n",
    "# with open('model_training.py', 'w', encoding='utf-8') as f:\n",
    "#     f.write(script)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11cc7a49-eea3-4d48-89c6-c54c699b5bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('hyperparameters.json', 'r') as f:\n",
    "        hyperparameters = json.load(f)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    logging.info('file not found')\n",
    "    raise\n",
    "\n",
    "except json.JSONDecodeError:\n",
    "    logging.info('json decode error')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b58e2b82-9f66-46a7-b78f-76eeef0283a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_training import EmbeddingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8bf25ca-a5b8-4020-ac24-0e6553dbb2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3848/466362735.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EmbeddingClassifier(\n",
       "  (fc1): Linear(in_features=768, out_features=225, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=225, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 768\n",
    "hidden_size = 225\n",
    "output_size = 2  \n",
    "\n",
    "\n",
    "model = EmbeddingClassifier(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f690d24f-5350-4ea7-af83-2abceffc7f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('df0_with_embeddings.csv')\n",
    "def convert_to_array(embedding_str):\n",
    "    embedding_list = embedding_str.replace('[', '').replace(']', '').split()\n",
    "    return np.array(embedding_list, dtype=float)\n",
    "\n",
    "df['embeddings'] = df['embeddings'].apply(convert_to_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cd44a7c-c24b-4798-8d3d-a70e35b47d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_embedding = df['embeddings'][0]\n",
    "# single_embedding_tensor = torch.tensor(single_embedding).unsqueeze(0).float()  # Convert to float32\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = model(single_embedding_tensor)\n",
    "#     probabilities = torch.softmax(output, dim=1)\n",
    "#     predicted_class = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "# predicted_class = predicted_class.item()\n",
    "# print(f\"Predicted class for the first embedding: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e6ad9-8f68-44b3-a1e5-ed2399b448bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81ade054-4bdb-463f-b183-2d51381e63d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritik/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Text: stock price apple inc. ( AAPL ) be expect rise next two year .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                 | 0/1 [00:00<?, ?it/s]2024-10-01 12:49:41,532 - INFO - Processing batch 1 with 1 texts.\n",
      "2024-10-01 12:49:42,057 - INFO - Batch 1 processed successfully.\n",
      "Processing batches: 100%|█████████████████████████| 1/1 [00:00<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings Shape: torch.Size([1, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import unicodedata\n",
    "import contractions\n",
    "from nltk import pos_tag\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, model_name='yiyanghkust/finbert-tone', custom_stopwords=None):\n",
    "        self.custom_stopwords = set(custom_stopwords) if custom_stopwords else set([\n",
    "            'a', 'an', 'the', 'and', 'but', 'or', 'if', 'because', 'as', 'while', \n",
    "            'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', \n",
    "            'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', \n",
    "            'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "            'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', \n",
    "            'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', \n",
    "            'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "            'too', 'very'\n",
    "        ])\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def lowercase_text(self, text):\n",
    "        def process_word(word):\n",
    "            if 2 <= len(word) <= 10 and word.isupper():\n",
    "                return word\n",
    "            else:\n",
    "                return word.lower()\n",
    "        \n",
    "        words_in_text = text.split()\n",
    "        processed_words = [process_word(word) for word in words_in_text]\n",
    "        \n",
    "        return ' '.join(processed_words)\n",
    "\n",
    "    def remove_punctuation(self, text, keep_punctuation=None):\n",
    "        if keep_punctuation is None:\n",
    "            keep_punctuation = {'.', ',', ':', '-', \"'\", '(', ')', '/', '?', '!'}\n",
    "            \n",
    "        text = re.sub(r'[\\n\\t]', ' ', text)\n",
    "        pattern = r'[^\\w\\s' + ''.join(re.escape(p) for p in keep_punctuation) + r']'\n",
    "        text = re.sub(pattern, ' ', text)\n",
    "        return text\n",
    "\n",
    "    def remove_html_urls(self, s):\n",
    "        try:\n",
    "            if '<' in s and '>' in s:\n",
    "                soup = BeautifulSoup(s, 'html.parser')\n",
    "                cleaned_text = soup.get_text(separator=' ')\n",
    "            else:\n",
    "                cleaned_text = s\n",
    "\n",
    "            url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "            cleaned_text = re.sub(url_pattern, '', cleaned_text)\n",
    "\n",
    "            return cleaned_text\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in remove_html_urls: {str(e)}\")\n",
    "            return s\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        try:\n",
    "            expanded_text = contractions.fix(text)\n",
    "            return expanded_text\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in expand_contractions: {str(e)}\")\n",
    "            return text\n",
    "\n",
    "    def remove_extra_whitespace(self, text):\n",
    "        try:\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in remove_extra_whitespace: {str(e)}\")\n",
    "            return text\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = word_tokenize(text)\n",
    "        filtered_words = [word for word in words if word.lower() not in self.custom_stopwords]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def normalize_text(self, text):\n",
    "        try:\n",
    "            normalized_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "            return normalized_text\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in normalize_text: {str(e)}\")\n",
    "            return text\n",
    "\n",
    "    def lemmatizer_with_pos(self, s):\n",
    "        try:\n",
    "            words = word_tokenize(s)\n",
    "            tagged_words = pos_tag(words)\n",
    "            lemmatized_words = []\n",
    "            \n",
    "            for word, tag in tagged_words:\n",
    "                wn_pos = self.get_wordnet_pos(tag) or 'n'\n",
    "                lemmatized_word = self.lemmatizer.lemmatize(word, pos=wn_pos)\n",
    "                lemmatized_words.append(lemmatized_word)\n",
    "            \n",
    "            return ' '.join(lemmatized_words)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in lemmatizer_with_pos: {str(e)}\")\n",
    "            return s\n",
    "\n",
    "    def get_wordnet_pos(self, tag):\n",
    "        if tag.startswith('J'):\n",
    "            return 'a'\n",
    "        elif tag.startswith('V'):\n",
    "            return 'v'\n",
    "        elif tag.startswith('N'):\n",
    "            return 'n'\n",
    "        elif tag.startswith('R'):\n",
    "            return 'r'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        try:\n",
    "            if not text or text.strip() == '':\n",
    "                return ''\n",
    "            \n",
    "            text = self.expand_contractions(text)\n",
    "            text = self.remove_html_urls(text)\n",
    "            text = self.remove_punctuation(text)\n",
    "            text = self.remove_extra_whitespace(text)\n",
    "            text = self.normalize_text(text)\n",
    "            text = self.lemmatizer_with_pos(text)\n",
    "            text = self.remove_stopwords(text)\n",
    "            text = self.lowercase_text(text)\n",
    "            \n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during preprocessing: {str(e)}\")\n",
    "            return text\n",
    "\n",
    "    def generate_embeddings(self, texts, batch_size=32, max_length=512):\n",
    "        all_embeddings = []\n",
    "        num_batches = (len(texts) + batch_size - 1) // batch_size \n",
    "\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "            try:\n",
    "                batch_texts = texts[i:i + batch_size]\n",
    "                logging.info(f\"Processing batch {i // batch_size + 1} with {len(batch_texts)} texts.\")\n",
    "\n",
    "                inputs = self.tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(**inputs)\n",
    "\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "                all_embeddings.append(batch_embeddings)\n",
    "\n",
    "                logging.info(f\"Batch {i // batch_size + 1} processed successfully.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing batch {i // batch_size + 1}: {e}\")\n",
    "\n",
    "        all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "        return all_embeddings\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    text_preprocessor = TextPreprocessor()\n",
    "    sample_text = \"The stock price of Apple Inc. (AAPL) is expected to rise in the next two years.\"\n",
    "    \n",
    "    processed_text = text_preprocessor.preprocess_text(sample_text)\n",
    "    print(f\"Processed Text: {processed_text}\")\n",
    "    \n",
    "    texts = [sample_text]\n",
    "    embeddings = text_preprocessor.generate_embeddings(texts)\n",
    "    print(f\"Embeddings Shape: {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dd79b02-0723-4166-89d7-54328f7579ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9148b4d3-1dba-4712-95fd-aeb4a82ae2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "# import praw\n",
    "# import torch\n",
    "# import logging\n",
    "\n",
    "\n",
    "# BATCH_SIZE = 100\n",
    "# MAX_ROWS = 1000\n",
    "\n",
    "\n",
    "# def init_reddit_client():\n",
    "#     try:\n",
    "#         reddit = praw.Reddit(\n",
    "#             client_id='CudIQ0m7vQerzS0wAyQzIA',\n",
    "#             client_secret='llSZVoIpkbt2LAnBDIXtNeTreuCdzg',\n",
    "#             user_agent='ubuntu:RedditScraper:v1.0 (by /u/Playful-Jellyfish834)'\n",
    "#         )\n",
    "#         logging.info(\"Initialized Reddit client successfully.\")\n",
    "#         return reddit\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error initializing Reddit client: {e}\")\n",
    "#         raise\n",
    "\n",
    "\n",
    "# def scrape_reddit_data(reddit, subreddit_names):\n",
    "#     all_texts = []\n",
    "#     dataset_file = 'reddit_data.json'\n",
    "\n",
    "\n",
    "#     if os.path.exists(dataset_file):\n",
    "#         try:\n",
    "#             with open(dataset_file, 'r') as f:\n",
    "#                 all_texts = json.load(f)\n",
    "#                 logging.info(f\"Loaded existing dataset with {len(all_texts)} texts.\")\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error loading dataset: {e}\")\n",
    "\n",
    "\n",
    "#     for subreddit_name in subreddit_names:\n",
    "#         try:\n",
    "#             subreddit = reddit.subreddit(subreddit_name)\n",
    "#             logging.info(f\"Scraping subreddit: {subreddit_name}\")\n",
    "\n",
    "#             # print('aaa')\n",
    "#             for submission in subreddit.new(limit=BATCH_SIZE):\n",
    "#                 if len(all_texts) >= MAX_ROWS:\n",
    "#                     logging.info(\"Reached maximum rows limit.\")\n",
    "#                     break\n",
    "\n",
    "\n",
    "#                 text = submission.title + \" \" + submission.selftext\n",
    "#                 all_texts.append(text)\n",
    "\n",
    "#             print('bbbb')\n",
    "#             for i in range(0, len(all_texts), BATCH_SIZE):\n",
    "#                 if len(all_texts) >= MAX_ROWS:\n",
    "#                     break\n",
    "                \n",
    "#                 batch_texts = all_texts[i:i + BATCH_SIZE]\n",
    "#                 processed_texts = [text_preprocessor.preprocess_text(text) for text in batch_texts]\n",
    "\n",
    "#                 embeddings = text_preprocessor.generate_embeddings(processed_texts)\n",
    "\n",
    "#                 for idx, embedding in enumerate(embeddings):\n",
    "#                     try:\n",
    "#                         single_embedding_tensor = torch.tensor(embedding).unsqueeze(0).float()  # Convert to float32\n",
    "\n",
    "#                         model.eval()\n",
    "#                         with torch.no_grad():\n",
    "#                             output = model(single_embedding_tensor)\n",
    "#                             probabilities = torch.softmax(output, dim=1)\n",
    "#                             predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "#                         if predicted_class == 1:\n",
    "#                             if len(all_texts) < MAX_ROWS:\n",
    "#                                 relevant_text = processed_texts[idx]\n",
    "#                                 all_texts[i + idx] = relevant_text \n",
    "#                                 logging.info(f\"Stored relevant text from batch {i // BATCH_SIZE}: {relevant_text[:30]}...\")\n",
    "\n",
    "#                     except Exception as e:\n",
    "#                         logging.error(f\"Error processing embedding at index {idx}: {e}\")\n",
    "\n",
    "#             try:\n",
    "#                 with open(dataset_file, 'w') as f:\n",
    "#                     json.dump(all_texts[:MAX_ROWS], f) \n",
    "#                     logging.info(f\"Saved dataset with {len(all_texts[:MAX_ROWS])} texts to {dataset_file}.\")\n",
    "#             except Exception as e:\n",
    "#                 logging.error(f\"Error saving dataset: {e}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error scraping subreddit {subreddit_name}: {e}\")\n",
    "\n",
    "#     try:\n",
    "#         with open(dataset_file, 'w') as f:\n",
    "#             json.dump(all_texts[:MAX_ROWS], f)  # Ensure final save respects MAX_ROWS\n",
    "#             logging.info(f\"Final save completed. Total relevant texts stored: {len(all_texts[:MAX_ROWS])}\")\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error during final save: {e}\")\n",
    "\n",
    "#     return all_texts[:MAX_ROWS]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2fa9f9f-378c-42c4-95cb-dbe8cc0bacbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72ed63da-7ffb-4354-a1c9-29cc31e541d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import praw\n",
    "import torch\n",
    "import logging\n",
    "import time\n",
    "import prawcore\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "MAX_ROWS = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63120db1-04e5-48a6-b58a-dec49cfbdfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_reddit_client():\n",
    "    try:\n",
    "        reddit = praw.Reddit(\n",
    "            client_id='CudIQ0m7vQerzS0wAyQzIA',\n",
    "            client_secret='llSZVoIpkbt2LAnBDIXtNeTreuCdzg',\n",
    "            user_agent='ubuntu:RedditScraper:v1.0 (by /u/Playful-Jellyfish834)'\n",
    "        )\n",
    "        logging.info(\"Initialized Reddit client successfully.\")\n",
    "        return reddit\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error initializing Reddit client: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c42e7d48-a737-4f6a-9d52-3c7891d1c03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_data(dataset_file='reddit_data.json'):\n",
    "    all_texts = []\n",
    "    \n",
    "    if os.path.exists(dataset_file):\n",
    "        try:\n",
    "            with open(dataset_file, 'r') as f:\n",
    "                all_texts = json.load(f)\n",
    "                logging.info(f\"Loaded existing dataset with {len(all_texts)} texts.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading dataset: {e}\")\n",
    "    \n",
    "    return all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9eb6a476-25be-437c-8371-9278a78ba088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(all_texts, dataset_file='reddit_data.json'):\n",
    "    try:\n",
    "        with open(dataset_file, 'w') as f:\n",
    "            json.dump(all_texts[:MAX_ROWS], f)\n",
    "            logging.info(f\"Saved dataset with {len(all_texts[:MAX_ROWS])} texts to {dataset_file}.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f836ce81-23ad-4f39-8865-db3fa29ffd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reddit_data(reddit, subreddit_names):\n",
    "    all_texts = load_existing_data()\n",
    "\n",
    "    for subreddit_name in subreddit_names:\n",
    "        try:\n",
    "            subreddit = reddit.subreddit(subreddit_name)\n",
    "            logging.info(f\"Scraping subreddit: {subreddit_name}\")\n",
    "            \n",
    "            for submission in subreddit.hot(limit=BATCH_SIZE):\n",
    "                if len(all_texts) >= MAX_ROWS:\n",
    "                    logging.info(\"Reached maximum rows limit.\")\n",
    "                    break\n",
    "\n",
    "                text = submission.title + \" \" + submission.selftext\n",
    "\n",
    "                submission.comments.replace_more(limit=0)\n",
    "                best_reply = \"\"\n",
    "                if submission.comments:\n",
    "                    best_reply = submission.comments[0].body  # Assuming first comment is the best\n",
    "\n",
    "                all_texts.append({\"text\": text, \"best_reply\": best_reply})\n",
    "\n",
    "            # Process the batches of texts and best replies\n",
    "            process_batches(all_texts)\n",
    "\n",
    "            save_data(all_texts)\n",
    "            time.sleep(2)\n",
    "\n",
    "        except prawcore.exceptions.NotFound:\n",
    "            logging.error(f\"Subreddit {subreddit_name} not found or inaccessible.\")\n",
    "        except prawcore.exceptions.Forbidden:\n",
    "            logging.error(f\"Access to subreddit {subreddit_name} is forbidden.\")\n",
    "        except prawcore.exceptions.RequestException as e:\n",
    "            logging.error(f\"Request error while accessing {subreddit_name}: {e}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error scraping subreddit {subreddit_name}: {e}\")\n",
    "\n",
    "    save_data(all_texts)\n",
    "\n",
    "    return all_texts[:MAX_ROWS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21c1b6ac-322d-4cb3-b2e1-d25b41d55b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batches(all_texts):\n",
    "    for i in range(0, len(all_texts), BATCH_SIZE):\n",
    "        if len(all_texts) >= MAX_ROWS:\n",
    "            break\n",
    "\n",
    "        batch_texts = all_texts[i:i + BATCH_SIZE]\n",
    "        processed_texts = [text_preprocessor.preprocess_text(item[\"text\"]) for item in batch_texts]\n",
    "        processed_replies = [text_preprocessor.preprocess_text(item[\"best_reply\"]) for item in batch_texts]\n",
    "\n",
    "        # Generate embeddings for both texts and replies\n",
    "        text_embeddings = text_preprocessor.generate_embeddings(processed_texts)\n",
    "        reply_embeddings = text_preprocessor.generate_embeddings(processed_replies)\n",
    "\n",
    "        for idx, (text_embedding, reply_embedding) in enumerate(zip(text_embeddings, reply_embeddings)):\n",
    "            try:\n",
    "                # Process the text embedding\n",
    "                single_text_embedding = torch.tensor(text_embedding).unsqueeze(0).float()  # Convert to float32\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    text_output = model(single_text_embedding)\n",
    "                    text_probabilities = torch.softmax(text_output, dim=1)\n",
    "                    text_predicted_class = torch.argmax(text_probabilities, dim=1).item()\n",
    "\n",
    "                # Process the reply embedding\n",
    "                single_reply_embedding = torch.tensor(reply_embedding).unsqueeze(0).float()  # Convert to float32\n",
    "                with torch.no_grad():\n",
    "                    reply_output = model(single_reply_embedding)\n",
    "                    reply_probabilities = torch.softmax(reply_output, dim=1)\n",
    "                    reply_predicted_class = torch.argmax(reply_probabilities, dim=1).item()\n",
    "\n",
    "                # Store relevant text and reply if their predicted class is relevant\n",
    "                if text_predicted_class == 1 or reply_predicted_class == 1:\n",
    "                    if len(all_texts) < MAX_ROWS:\n",
    "                        relevant_text = processed_texts[idx]\n",
    "                        relevant_reply = processed_replies[idx]\n",
    "                        all_texts[i + idx] = {\"text\": relevant_text, \"best_reply\": relevant_reply}\n",
    "                        logging.info(f\"Stored relevant text and reply from batch {i // BATCH_SIZE}: {relevant_text[:30]}..., {relevant_reply[:30]}...\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing embedding at index {idx}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03439fff-f22b-4b2c-9de5-a1e35a1e3d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a93f225-8362-4e1b-8bbd-e7936d97df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # Initialize text preprocessor and model (assuming they're already defined elsewhere)\n",
    "    text_preprocessor = TextPreprocessor()  # Placeholder for loading your preprocessor\n",
    "\n",
    "    reddit_client = init_reddit_client()\n",
    "\n",
    "    financial_subreddits = [\n",
    "        \"stocks\",\n",
    "        \"investing\",\n",
    "        \"personalfinance\",\n",
    "        \"finance\",\n",
    "        \"StockMarket\",\n",
    "        \"valueinvesting\",\n",
    "        \"financialindependence\",\n",
    "        \"ETF\",\n",
    "        \"dividends\",\n",
    "        \"business\",\n",
    "        \"Economics\",\n",
    "        \"investing_discussion\"\n",
    "    ]\n",
    "\n",
    "    scraped_data = scrape_reddit_data(reddit_client, financial_subreddits)\n",
    "    print(f\"Scraping completed. Total relevant texts stored: {len(scraped_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801cb1a0-ce13-423c-bb44-0a0a61c7a5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
